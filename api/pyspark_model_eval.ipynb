{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "!unzip cvModel1.zip\n",
        "!unzip pipelineFit.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjQrX8nj8bK1",
        "outputId": "f71eecc7-e168-4939-8dd6-300bcbc4c500"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=0ac105f8c300e49bc4d44387504a3bf1244f840d95eb207e5dc9bc5e04daed54\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n",
            "Archive:  cvModel1.zip\n",
            "   creating: cvModel1/\n",
            "   creating: cvModel1/bestModel/\n",
            "   creating: cvModel1/bestModel/data/\n",
            " extracting: cvModel1/bestModel/data/.part-00000-e0e6fbd7-dd48-4880-82b5-6e2637cd0c07-c000.snappy.parquet.crc  \n",
            " extracting: cvModel1/bestModel/data/._SUCCESS.crc  \n",
            "  inflating: cvModel1/bestModel/data/part-00000-e0e6fbd7-dd48-4880-82b5-6e2637cd0c07-c000.snappy.parquet  \n",
            " extracting: cvModel1/bestModel/data/_SUCCESS  \n",
            "   creating: cvModel1/bestModel/metadata/\n",
            " extracting: cvModel1/bestModel/metadata/.part-00000.crc  \n",
            " extracting: cvModel1/bestModel/metadata/._SUCCESS.crc  \n",
            "  inflating: cvModel1/bestModel/metadata/part-00000  \n",
            " extracting: cvModel1/bestModel/metadata/_SUCCESS  \n",
            "   creating: cvModel1/estimator/\n",
            "   creating: cvModel1/estimator/metadata/\n",
            " extracting: cvModel1/estimator/metadata/.part-00000.crc  \n",
            " extracting: cvModel1/estimator/metadata/._SUCCESS.crc  \n",
            "  inflating: cvModel1/estimator/metadata/part-00000  \n",
            " extracting: cvModel1/estimator/metadata/_SUCCESS  \n",
            "   creating: cvModel1/evaluator/\n",
            "   creating: cvModel1/evaluator/metadata/\n",
            " extracting: cvModel1/evaluator/metadata/.part-00000.crc  \n",
            " extracting: cvModel1/evaluator/metadata/._SUCCESS.crc  \n",
            "  inflating: cvModel1/evaluator/metadata/part-00000  \n",
            " extracting: cvModel1/evaluator/metadata/_SUCCESS  \n",
            "   creating: cvModel1/metadata/\n",
            " extracting: cvModel1/metadata/.part-00000.crc  \n",
            " extracting: cvModel1/metadata/._SUCCESS.crc  \n",
            "  inflating: cvModel1/metadata/part-00000  \n",
            " extracting: cvModel1/metadata/_SUCCESS  \n",
            "Archive:  pipelineFit.zip\n",
            "   creating: pipelineFit/\n",
            "   creating: pipelineFit/metadata/\n",
            " extracting: pipelineFit/metadata/.part-00000.crc  \n",
            " extracting: pipelineFit/metadata/._SUCCESS.crc  \n",
            "  inflating: pipelineFit/metadata/part-00000  \n",
            " extracting: pipelineFit/metadata/_SUCCESS  \n",
            "   creating: pipelineFit/stages/\n",
            "   creating: pipelineFit/stages/0_RegexTokenizer_f89387e588d2/\n",
            "   creating: pipelineFit/stages/0_RegexTokenizer_f89387e588d2/metadata/\n",
            " extracting: pipelineFit/stages/0_RegexTokenizer_f89387e588d2/metadata/.part-00000.crc  \n",
            " extracting: pipelineFit/stages/0_RegexTokenizer_f89387e588d2/metadata/._SUCCESS.crc  \n",
            "  inflating: pipelineFit/stages/0_RegexTokenizer_f89387e588d2/metadata/part-00000  \n",
            " extracting: pipelineFit/stages/0_RegexTokenizer_f89387e588d2/metadata/_SUCCESS  \n",
            "   creating: pipelineFit/stages/1_CountVectorizer_48b194aaea07/\n",
            "   creating: pipelineFit/stages/1_CountVectorizer_48b194aaea07/data/\n",
            " extracting: pipelineFit/stages/1_CountVectorizer_48b194aaea07/data/.part-00000-09a14446-67b1-4d67-890d-eb201258ee10-c000.snappy.parquet.crc  \n",
            " extracting: pipelineFit/stages/1_CountVectorizer_48b194aaea07/data/._SUCCESS.crc  \n",
            "  inflating: pipelineFit/stages/1_CountVectorizer_48b194aaea07/data/part-00000-09a14446-67b1-4d67-890d-eb201258ee10-c000.snappy.parquet  \n",
            " extracting: pipelineFit/stages/1_CountVectorizer_48b194aaea07/data/_SUCCESS  \n",
            "   creating: pipelineFit/stages/1_CountVectorizer_48b194aaea07/metadata/\n",
            " extracting: pipelineFit/stages/1_CountVectorizer_48b194aaea07/metadata/.part-00000.crc  \n",
            " extracting: pipelineFit/stages/1_CountVectorizer_48b194aaea07/metadata/._SUCCESS.crc  \n",
            "  inflating: pipelineFit/stages/1_CountVectorizer_48b194aaea07/metadata/part-00000  \n",
            " extracting: pipelineFit/stages/1_CountVectorizer_48b194aaea07/metadata/_SUCCESS  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SQLContext\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, Word2Vec, Tokenizer, StopWordsRemover\n",
        "from pyspark.ml.classification import LogisticRegression, NaiveBayes, LinearSVC, OneVsRest, MultilayerPerceptronClassifier\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.feature import HashingTF, IDF\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
        "from pyspark.ml.pipeline import PipelineModel"
      ],
      "metadata": {
        "id": "VaQkv7zV5XkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc =SparkContext()\n",
        "sqlContext = SQLContext(sc)"
      ],
      "metadata": {
        "id": "NusJeUrl5iUV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2a11fc-8f53-443f-b172-86843433b1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "customSchema = StructType([\n",
        "    StructField(\"label\", IntegerType()),\n",
        "    StructField(\"text\", StringType())])"
      ],
      "metadata": {
        "id": "9OjDFFbE7azX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_val = sqlContext.read.format(\"csv\").option(\"header\", \"true\").schema(customSchema).load('cleaned_twitter_validation.csv')"
      ],
      "metadata": {
        "id": "s2OnVtQw5lDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regexTokenizer = RegexTokenizer(inputCol=\"text\", outputCol=\"tokens\", pattern=r\" +\")\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokens\")\n",
        "countVectors = CountVectorizer(inputCol=\"tokens\", outputCol=\"features\", vocabSize=15000, minDF=5)\n",
        "word2Vec = Word2Vec(vectorSize=100, minCount=0,maxIter=20, inputCol=\"tokens\", outputCol=\"features\")\n",
        "hashingTF = HashingTF(inputCol=\"tokens\", outputCol=\"rawFeatures\", numFeatures=15000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=10) #minDocFreq: remove sparse terms"
      ],
      "metadata": {
        "id": "qCiZYrow5l8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the pipeline to validation documents.\n",
        "pipeline = PipelineModel.load('pipelineFit')\n",
        "dataset_val = pipeline.transform(df_val)\n",
        "dataset_val.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1qYnMXM5nkA",
        "outputId": "51ba27a0-0b0c-4eb7-ef6e-c2e344d06617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+\n",
            "|label|                text|              tokens|            features|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "|    0|mentioned faceboo...|[mentioned, faceb...|(13363,[3,16,23,2...|\n",
            "|    2|bbc news amazon b...|[bbc, news, amazo...|(13363,[2,34,138,...|\n",
            "|    1|why pay  word  fu...|[why, pay, word, ...|(13363,[90,265,69...|\n",
            "|    1|csgo matchmaking ...|[csgo, matchmakin...|(13363,[0,115,262...|\n",
            "|    2|now  president sl...|[now, president, ...|(13363,[7,32,143,...|\n",
            "+-----+--------------------+--------------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cvModel = CrossValidatorModel.load('cvModel1')\n",
        "predictions = cvModel.transform(dataset_val)"
      ],
      "metadata": {
        "id": "SXZ3y2li5sxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions.filter(predictions['prediction'] == 0) \\\n",
        "    .select(\"text\",\"probability\",\"label\",\"prediction\") \\\n",
        "    .orderBy(\"probability\", ascending=False) \\\n",
        "    .show(n = 10, truncate = 110)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGU1Ae-Z9gwr",
        "outputId": "ddbae940-619b-44d3-cac1-73762a6f2af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----+----------+\n",
            "|                                                                                                          text|                                                                           probability|label|prediction|\n",
            "+--------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----+----------+\n",
            "|words gratitude trevorphilipsstanaccount love   gta tumblr fandom particular   creating  talented  beautifu...|   [0.9988016127239734,8.530927674802259E-6,6.289466453168379E-4,5.609097030348915E-4]|    0|       0.0|\n",
            "|the nigeria national team   ranked  29th best team  world  3rd africa  latest fifa world rankings  its  fir...| [0.9981446665691251,1.9505353884937112E-5,0.0017476617390177118,8.816633797230984E-5]|    0|       0.0|\n",
            "|mentioned facebook   struggling  motivation  run   day    translated toms great auntie hayley cant get  bed...|   [0.9977606936493543,3.7247591183736E-4,0.0018528021326134783,1.4028306195030467E-5]|    0|       0.0|\n",
            "|       sound enjoy  groove   little montage made tribute  desert eagle   powerful badass secondary weapon ever| [0.9976589706966389,2.8788393965020206E-4,6.845662411592804E-4,0.0013685791225514698]|    0|       0.0|\n",
            "|fab seeing  bungalows built member  support  many others highlights  seeing family  andrew bibby   spade tr...|[0.9965912371425939,0.0013858363888398487,0.0016229589325094528,3.9996753605653595E-4]|    0|       0.0|\n",
            "|                                                        ban  battlefield player gariblak  occurred see details|    [0.9954765517869245,0.00158452509463051,0.001284855441318058,0.001654067677126903]|    0|       0.0|\n",
            "|pleased announce  names  referees approved  fifa referees committee   2020 international lists bondo joshua...|    [0.9939714365750146,0.00393837132260728,0.001800278287908446,2.899138144697024E-4]|    0|       0.0|\n",
            "|lol set  string tinder dates  back  would  bail  without messaging  figured would save   disappointment mad...|   [0.9933597371914479,0.005789867036327121,1.602995964408091E-4,6.900961757842067E-4]|    0|       0.0|\n",
            "|         far best stream without raid  hovering around 1215 viewers   hour  gained boat load follows much love|  [0.9930650724148051,5.417010939037415E-4,0.0020852469715993715,0.004307979519691763]|    0|       0.0|\n",
            "|stupid little fucking baby children   fullgrown adults copypasted microsoft paint  squeegeeed  disproportio...|[0.9920356490683505,0.0032517347502669077,0.0035881228983950858,0.0011244932829875894]|    0|       0.0|\n",
            "+--------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
        "evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1k3mEcT6gy3",
        "outputId": "ec259584-898f-4c5f-98b9-24d25d24e427"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9037828332690099"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}